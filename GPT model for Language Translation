import tensorflow as tf
import pathlib

text_file = tf.keras.utils.get_file(
    fname = 'fra-eng.zip',
    origin = 'https://storage.googleapis.com/download.tensorflow.org/data/fra-eng.zip',
    extract = True,
)

extracted_dir_name = pathlib.Path(text_file).stem
text_file = pathlib.Path(text_file).parent / extracted_dir_name / 'fra.txt'
print(text_file)

with open(text_file) as fp:
  text_pair = [line for line in fp]

import random
for _ in range(5):

  print(random.choice(text_pair))

import unicodedata
import re

def normalize(line):
  line = unicodedata.normalize('NFKC',line.strip().lower())
  line = re.sub(r"^([^ \w])(?!\s)",r"\1",line)
  line = re.sub(r"(\s[^ \w])(?!\s)",r"\1",line)
  line = re.sub(r"(?!\s)([^ \w])$",r"\1",line)
  line = re.sub(r"(?!\s)([^ \w]\s)",r"\1",line)
  eng, fre = line.split('\t')
  fre = '[start]' + fre + '[end]'
  return eng, fre

with open(text_file) as fp:
  text_pairs = [normalize(line) for line in fp]

for _ in range(5):

  print(random.choice(text_pairs))

eng_tokens, fre_tokens = set(), set()
eng_maxlen, fre_maxlen = 0,0
for eng, fre in text_pairs:
  eng_token, fre_token = eng.split(), fre.split()
  eng_maxlen = max(eng_maxlen,len(eng_token))
  fre_maxlen = max(fre_maxlen,len(fre_token))
  eng_tokens.update(eng_token)
  fre_tokens.update(fre_token)

print(f"total token in english {len(eng_tokens)}")
print(f"total token in french {len(fre_tokens)}")
print(f"max length of line is {eng_maxlen}")
print(f"max length of line is  {fre_maxlen}")

import pickle

with open("text_pairs.pickle","wb") as fp:
  pickle.dump(text_pairs,fp)

from  tensorflow.keras.layers import TextVectorization

import pickle

with open("text_pairs.pickle","rb") as fp:
  text_pairs = pickle.load(fp)

random.shuffle(text_pairs)

n_val = int(.15*len(text_pairs))
n_train = len(text_pairs)-2*n_val
train_pair = text_pairs[:n_train]
test_pair = text_pairs[n_train+n_val:]

vocab_en = 10000
vocab_fr = 20000
seq_length = 25


eng_vect = TextVectorization(
    max_tokens = vocab_en,
    standardize = None,
    split ='whitespace',
    output_mode ='int',
    output_sequence_length = seq_length,
)

fre_vect = TextVectorization(
    max_tokens = vocab_fr,
    standardize = None,
    split = 'whitespace',
    output_mode = 'int',
    output_sequence_length = seq_length + 1
)

train_eng = [pair[0] for pair in train_pair]
train_fre = [pair[1] for pair in train_pair]

eng_vect.adapt(train_eng)
fre_vect.adapt(train_fre)
with open('vectorize.pickle','wb') as fp:
  data = {'train':train_pair,
          'test':test_pair,
          'eng_vect':eng_vect.get_config(),
          'fre_vect':fre_vect.get_config(),
          'eng_weights':eng_vect.get_weights(),
          'fre_weights':fre_vect.get_weights()
          }
  pickle.dump(data,fp)

with open('vectorize.pickle','rb') as fp:
  data = pickle.load(fp)

train_pair = data['train']
test_pair = data['test']

eng_vect = TextVectorization.from_config(data['eng_vect'])
fre_vect = TextVectorization.from_config(data['fre_vect'])

eng_vect.set_weights(data['eng_weights'])
fre_vect.set_weights(data['fre_weights'])

train_eng = [pair[0] for pair in train_pair]
train_fre = [pair[1] for pair in train_pair]

eng_vect.adapt(train_eng)
fre_vect.adapt(train_fre)

def format_dataset(eng,fre):
  eng = eng_vect(eng)
  fre = fre_vect(fre)

  source = {'enc_input':eng,
            'dec_input':fre[:,:-1]
            }

  target = fre[:,1:]
  return (source,target)

def make_dataset(pairs,batchsize = 64):
    eng_text, fre_text = zip(*pairs)
    dataset = tf.data.Dataset.from_tensor_slices((list(eng_text),list(fre_text)))
    return dataset.shuffle(2048).batch(batchsize).map(format_dataset).prefetch(16).cache()

train_ds = make_dataset(train_pair)
for inputs, target in train_ds.take(1):
  print(inputs['enc_input'].shape)
  print(inputs['enc_input'][0])
  print(inputs['dec_input'].shape)
  print(inputs['dec_input'][0])
  print(target.shape)
  print(target[0])

test_ds = make_dataset(test_pair)

import numpy as np
import tensorflow as tf
def pos_enc_matrix(L,d,n=10000):
  assert d%2 ==0
  d2 = d//2

  p = np.zeros((L,d))
  k = np.arange(L).reshape(-1,1)
  i = np.arange(d2).reshape(1,-1)

  denom = np.power(n, -i/d2)
  args = k * denom

  p[:, ::2] = np.sin(args)
  p[:, 1::2] = np.cos(args)
  return p

class PositionalEmbedding(tf.keras.layers.Layer):
      def __init__(self,seq_length,vocal_size,embed_dim, **kwargs):
        super().__init__(**kwargs)
        self.seq_length = seq_length
        self.vocal_size = vocal_size
        self.embed_dim = embed_dim

        self.token_embeddings = tf.keras.layers.Embedding(
            input_dim = vocal_size,
            output_dim = self.embed_dim,
            mask_zero = True
        )

        matrix = pos_enc_matrix(self.seq_length,self.embed_dim)
        self.position_embeddings = tf.constant(matrix,dtype = 'float32')

      def call(self,inputs):
        embedded_tokens = self.token_embeddings(inputs)
        return embedded_tokens + self.position_embeddings

      def compute_mask(self,*args, **kwargs):
        return self.token_embeddings.compute_mask(*args,**kwargs)

      def get_config(self):
        config = super().get_config()
        config.update({
            'seq_length':self.seq_length,
            'vocal_size':self.vocal_size,
            'embed_dim':self.embed_dim
        })
        return config


vocab_en = 10000
#vocab_fr = 20000
seq_length = 25

for inputs, targets in train_ds.take(1):
  print(inputs['enc_input'])
  embed_en = PositionalEmbedding(seq_length,vocab_en,embed_dim = 512)
  em_emb= embed_en(inputs['enc_input'])
  print(em_emb._keras_mask)

def self_attention(input_shape,prefix='att',mask = False, **kwargs):
  inputs = tf.keras.layers.Input(shape=input_shape,dtype='float32',name = f"{prefix}_in1")
  attention = tf.keras.layers.MultiHeadAttention(name = f"{prefix}_att1",**kwargs)
  norm = tf.keras.layers.LayerNormalization(name = f"{prefix}_norm1")
  add = tf.keras.layers.Add(name = f"{prefix}_add1")

  attout = attention(query = inputs, value = inputs, key = inputs,use_causal_mask = mask)
  output = norm(add([inputs,attout]))
  model = tf.keras.Model(inputs=inputs, outputs=output, name = f"{prefix}_att")
  return model

seq_length = 25
key_dim = 128
num_heads = 8

model = self_attention( input_shape=(seq_length,key_dim),num_heads=num_heads,key_dim=key_dim)
tf.keras.utils.plot_model(model)

def cross_attention(input_shape,context_shape,prefix='att',**kwargs):
  inputs = tf.keras.layers.Input(shape=input_shape,dtype='float32',name = f"{prefix}_in2")
  context = tf.keras.layers.Input(shape=context_shape, dtype='float32',name = f"{prefix}_ctx")
  attention = tf.keras.layers.MultiHeadAttention(name = f"{prefix}_att2",**kwargs)
  norm = tf.keras.layers.LayerNormalization(name = f"{prefix}_norm2")
  add = tf.keras.layers.Add(name = f"{prefix}_add2")

  attout = attention(query = inputs, value = context, key = context)
  output = norm(add([inputs,attout]))
  model = tf.keras.Model(inputs=[(context,inputs)], outputs=output, name = f"{prefix}_crs_att")
  return model

model =cross_attention(input_shape=(seq_length,key_dim),num_heads=num_heads,key_dim=key_dim,context_shape=(seq_length,key_dim))
tf.keras.utils.plot_model(model,show_shapes=True,show_dtype=True,show_layer_names=True,show_layer_activations=True)

def feed_forward(input_shape,model_dim,ff_dim,dropout = .1,prefix='ff'):
  inputs = tf.keras.layers.Input(shape=input_shape,dtype='float32',name = f"{prefix}_in3")
  dense1 = tf.keras.layers.Dense(ff_dim,activation='relu',name = f"{prefix}_ff1")
  dense2 = tf.keras.layers.Dense(model_dim,name = f"{prefix}_ff2")
  drop = tf.keras.layers.Dropout(dropout,name = f"{prefix}_drop")
  add = tf.keras.layers.Add(name = f"{prefix}_add3")

  ffout = drop(dense2(dense1(inputs)))

  norm = tf.keras.layers.LayerNormalization(name = f"{prefix}_norm3")
  output = norm(add([inputs,ffout]))
  model = tf.keras.Model(inputs=inputs, outputs=output, name = f"{prefix}_ff")
  return model

ff_dim = 512
model = feed_forward(input_shape=(seq_length,key_dim),model_dim=key_dim,ff_dim=ff_dim)
tf.keras.utils.plot_model(model)

import tensorflow as tf

import tensorflow as tf

def encoder(input_shape,key_dim,ff_dim,dropout = .1,prefix='enc',**kwargs):
  inputs = tf.keras.layers.Input(shape=input_shape,dtype='float32',name = f"{prefix}_in0")
  attention_output = self_attention(input_shape,prefix = prefix,key_dim=key_dim,mask  = False, **kwargs)(inputs)
  outputs = feed_forward(input_shape, key_dim, ff_dim, dropout,prefix )(attention_output)

  model = tf.keras.Model(inputs=inputs, outputs=outputs, name=f"{prefix}_encoder")
  return model

seq_length = 25
key_dim= 128
ff_dim= 512
num_heads= 8

model=encoder(input_shape= (seq_length, key_dim), key_dim=key_dim, ff_dim=ff_dim, num_heads=num_heads)

tf.keras.utils.plot_model(model,show_layer_names=True,show_dtype=True,show_shapes=True,show_layer_activations=True)

def decoder(input_shape,key_dim,ff_dim,dropout = .1,prefix='dec',**kwargs):
  inputs = tf.keras.layers.Input(shape=input_shape, dtype = 'float32',name = f"{prefix}_in0")
  context = tf.keras.layers.Input(shape=input_shape, dtype = 'float32',name = f"{prefix}_ctx0")
  att_model = self_attention(input_shape,prefix = prefix,key_dim=key_dim,mask  = False, **kwargs)
  cross_model = cross_attention(input_shape,input_shape,prefix = prefix,key_dim=key_dim, **kwargs)
  ff_model = feed_forward(input_shape, key_dim, ff_dim, dropout,prefix )

  x= att_model(inputs)
  x = cross_model([context,x])
  outputs = ff_model(x)

  model = tf.keras.Model(inputs=[(context,inputs)], outputs=outputs, name=prefix)
  return model

seq_length = 25
key_dim= 128
ff_dim= 512
num_heads= 8

model=decoder(input_shape= (seq_length, key_dim), key_dim=key_dim, ff_dim=ff_dim, num_heads=num_heads)

tf.keras.utils.plot_model(model,show_layer_names=True,show_dtype=True,show_shapes=True,show_layer_activations=True)

def transformer(num_layers,num_heads,seq_length,key_dim,ff_dim,vocab_size_en,vocab_size_fr,dropout=.1,name='transformer'):
  inputs_enc = tf.keras.layers.Input(shape=(seq_length,),dtype='float32',name='enc_input')
  inputs_dec = tf.keras.layers.Input(shape=(seq_length,),dtype='float32',name='dec_input')

  emb_enc = PositionalEmbedding(seq_length,vocab_size_en,key_dim,name='embed_enc')
  emb_dec = PositionalEmbedding(seq_length,vocab_size_fr,key_dim,name='embed_dec')

  encoders = [encoder(input_shape=(seq_length,key_dim), key_dim=key_dim,ff_dim=ff_dim,dropout=dropout,prefix=f'enc{i}',num_heads=num_heads) for i in range(num_layers)]
  decoders= [decoder(input_shape=(seq_length,key_dim), key_dim=key_dim,ff_dim=ff_dim,dropout=dropout,prefix=f'dec{i}',num_heads=num_heads) for i in range(num_layers)]

  final = tf.keras.layers.Dense(vocab_size_fr,name='linear')

  x1 = emb_enc(inputs_enc)
  x2 = emb_dec(inputs_dec)

  for layer in encoders:
    x1= layer(x1)
  for layer in decoders:
    x2= layer([x1,x2])

  output = final(x2)

  try:
    del output._keras_mask
  except:
    pass

  model = tf.keras.Model(inputs=[inputs_enc,inputs_dec],outputs=output,name=name)
  return model

seq_length=25
num_layers=4
num_heads=8
key_dim=128
ff_dim=512
dropout=.1
vocab_size_en=10000
vocab_size_fr=20000

model= transformer(num_layers,num_heads,seq_length,key_dim,ff_dim,vocab_size_en,vocab_size_fr,dropout)
tf.keras.utils.plot_model(model,show_shapes=True,show_dtype=True,show_layer_names=True,show_layer_activations=True)

model.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['accuracy'])
model.summary()

history = model.fit(train_ds,epochs=20,validation_data=test_ds)

class customSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):
  def __init__(self, key_dim, warmup_steps=40000):
    super().__init__()

    self.key_dim = key_dim
    self.warmup_steps = warmup_steps

    self.d = tf.cast(self.key_dim, tf.float32)

  def __call__(self,step):
      step = tf.cast(step, tf.float32)
      arg1 = tf.math.rsqrt(step)
      arg2 = step * (self.warmup_steps ** -1.5)

      return tf.math.rsqrt(self.d) * tf.math.minimum(arg1, arg2)

  def get_config(self):
      config = {
          'key_dim': self.key_dim,
          'warmup_steps': self.warmup_steps
      }
      return config

key_dim =128
lr = customSchedule(key_dim)
optimizer= tf.keras.optimizers.Adam(lr, beta_1=0.9, beta_2=0.98, epsilon=1e-9)

def masked_loss(label,pred):
  mask = label != 0

  loss_object = tf.keras.losses.SparseCategoricalCrossentropy(
      from_logits=True, reduction='none'
   )

  loss = loss_object(label, pred)

  mask = tf.cast(mask, dtype=loss.dtype)
  loss *= mask
  return tf.reduce_sum(loss)/tf.reduce_sum(mask)

def mask_accuracy(label,pred):
  pred = tf.argmax(pred,axis=2)
  label = tf.cast(label,pred.dtype)
  match = label == pred

  mask = label != 0
  match = match & mask

  match = tf.cast(match,dtype=tf.float32)
  mask = tf.cast(mask,dtype=tf.float32)
  return tf.reduce_sum(match)/tf.reduce_sum(mask)

model.compile(loss=masked_loss,optimizer=optimizer,metrics=[mask_accuracy])
model.summary()

history = model.fit(train_ds,epochs=20,validation_data=test_ds)

def translate(sentence):
  enc_tokens = eng_vect(sentence)
  enc_tokens = tf.expand_dims(enc_tokens, axis=0)
  lookup = list(fre_vect.get_vocabulary())
  start_sent, end_sent = '[start]','[end]'
  output_sent = [start_sent]

  for i in range(seq_length):
    vector = fre_vect([' '.join(output_sent)])
    assert vector.shape == (1,seq_length + 1)
    dec_tokens = vector[:, :-1]
    assert dec_tokens.shape == (1,seq_length)
    pred = model([enc_tokens,dec_tokens])
    assert pred.shape == (1,seq_length,vocab_size_fr)
    word = lookup[np.argmax(pred[0,i,:])]
    output_sent.append(word)
    if word == end_sent:
      break

  return output_sent


seq_length = 25
vocab_size_en = 10000
vocab_size_fr = 20000
test_count = 20

for n in range(test_count):
  eng_sent, fre_sent = random.choice(test_pair)
  trans = translate(eng_sent)

  print(f'Test case:{n}')
  print(f'English sentence: {eng_sent}')
  print(f'Translated sentence: {" ".join(trans)}')
  print(f'French sentence: {fre_sent}')




